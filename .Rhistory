# # look at clustering by lab
# ggplot( data = d,
#         aes( x = as.factor(lab),
#              y = Y ) ) +
#   geom_violin() +
#   theme_bw()
#
# # look at clustering by subject for first 10 subjects
# ggplot( data = d[ d$subid %in% 1:10, ],
#         aes( x = as.factor(subid),
#              y = Y ) ) +
#   geom_violin() +
#   theme_bw()
##### Analyze the Dataset #####
lmer.issue = NA
tryCatch({
m = lmer( Y ~ trial_type + (1|subid) + (1|lab),
data = d )
#warning("Fake warning")  # to test warning-handling
error("Fake error")
}, warning = function(warn){
lmer.issue <<- warn
}, error = function(err){
lmer.issue <<- err
m <<- NA
} )
if (!is.na(m)) {
# point estimate for ADS
bhat = fixef(m)["trial_typeADS"]
# inference for ADS
pval = coef(summary(m))["trial_typeADS", "Pr(>|t|)"]
if ( doCI == TRUE ) ci.width = diff( confint(m)["trial_typeADS",] ) else ci.width = NA
} else {
bhat = NA
pval = NA
doCI = NA
}
##### Return Everything #####
return( list(d = d,
res = data.frame(bhat = bhat,
pval = pval,
ci.width =  ci.width,
lmer.issue = as.character(lmer.issue),
singular = isSingular(m) ) ) )
}
# # sanity check
# fake = do_one_dataset(
debug(do_one_dataset)
# sanity check
fake = do_one_dataset( .b = b,
.V1 = V1,
.V2 = V2,
.s = s,
.nsubj = nsubj,
.nlab = nlab,
.ntrials = ntrials )
fake$res
lmer.issue
m
do_one_dataset = function(.b,
.V1,
.V2,
.s,
.nsubj,
.nlab,
.ntrials,
doCI = FALSE ) {  # CI profiling is slow
# .b = b
# .V1 = V1
# .V2 = V2
# .s = s
# .nsubj = 160
# .nlab = 10
# .ntrials = 8
# generate fixed parts of dataset
d <- expand.grid(subid = 1:.nsubj,
trial_pair = c(1:.ntrials),
trial_type = c("IDS", "ADS"))
d <- d %>% arrange(subid)
d$trial_num <- rep(c(1:16), each=1, .nsubj)
d$lab <- rep(c(1:.nlab), each = 256)
d$subid <- as.factor(d$subid)
d$trial_type <- as.factor(d$trial_type)
# generate subject intercepts
d = d %>% group_by(subid) %>%
mutate( sint = rnorm( n = 1,
mean = 0,
sd = sqrt(.V1) ) )
##### Generate the Dataset #####
# generate lab intercepts
# we're generating these independently from the subject intercepts,
#  so assuming diagonal correlation matrix of random intercepts
d = d %>% group_by(lab) %>%
mutate( lint = rnorm( n = 1,
mean = 0,
sd = sqrt(.V2) ) )
# # sanity check: should be close
# var( d$sint[ !duplicated( d$subid ) ] ); .V1
# var( d$lint[ !duplicated( d$lab ) ] ); .V2
# linear predictor
d$linpred = .b[1] + d$lint + d$lint +
(.b[2] * (d$trial_type == "IDS") )
d$Y = rnorm( n = nrow(d),
mean = d$linpred,
sd = .s )
# Sanity checks
# library(ggplot2)
#
# ggplot( data = d,
#         aes( x = subid,
#              y = Y,
#              color = as.factor(lab) ) ) +
#   geom_violin() +
#   theme_bw()
#
# # look at clustering by lab
# ggplot( data = d,
#         aes( x = as.factor(lab),
#              y = Y ) ) +
#   geom_violin() +
#   theme_bw()
#
# # look at clustering by subject for first 10 subjects
# ggplot( data = d[ d$subid %in% 1:10, ],
#         aes( x = as.factor(subid),
#              y = Y ) ) +
#   geom_violin() +
#   theme_bw()
##### Analyze the Dataset #####
lmer.issue = NA
tryCatch({
m = lmer( Y ~ trial_type + (1|subid) + (1|lab),
data = d )
#warning("Fake warning")  # to test warning-handling
error("Fake error")
}, error = function(err){
lmer.issue <<- err
m <<- NA
}, warning = function(warn){
lmer.issue <<- warn
} )
if (!is.na(m)) {
# point estimate for ADS
bhat = fixef(m)["trial_typeADS"]
# inference for ADS
pval = coef(summary(m))["trial_typeADS", "Pr(>|t|)"]
if ( doCI == TRUE ) ci.width = diff( confint(m)["trial_typeADS",] ) else ci.width = NA
} else {
bhat = NA
pval = NA
doCI = NA
}
##### Return Everything #####
return( list(d = d,
res = data.frame(bhat = bhat,
pval = pval,
ci.width =  ci.width,
lmer.issue = as.character(lmer.issue),
singular = isSingular(m) ) ) )
}
# sanity check
fake = do_one_dataset( .b = b,
.V1 = V1,
.V2 = V2,
.s = s,
.nsubj = nsubj,
.nlab = nlab,
.ntrials = ntrials )
fake$res
?globalVariables
library(devtools)
check()
check()
source('~/Dropbox/Personal computer/Independent studies/MetaUtility R package/MetaUtility/R/functions.R', echo=TRUE)
######## with clustering
setwd("~/Dropbox/Personal computer/Independent studies/2020/Meta-regression metrics (MRM)/Code (git)/Simulation study")
source("helper_MRM.R")
# re-source the updated fns in case of shared names
setwd("~/Dropbox/Personal computer/Independent studies/MetaUtility R package/MetaUtility/R")
source("functions.R")
library(dplyr)
library(ICC)
d = sim_data2(k = 30,
m = 20,
b0 = 0,
bc = 0,
bb = 0,
V = 0.25,
Vzeta = 0.2,
muN = 100,
minN = 50,
sd.w = 1,
true.effect.dist = "normal")
# change cluster name to not be the default inside prop_stronger
names(d)[names(d) == "cluster"] = "paper"
# accounting for clusters: CI = [0%, 61%]
Phat = prop_stronger( q = -0.1,
ci.level = 0.95,
estimate.method = "calibrated",
ci.method = "calibrated",
dat = d,
yi.name = "yi",
vi.name = "vyi",
tail = "below",
cluster.name = "paper")
cluster
cluster.name
datNest = dat %>% group_nest(cluster)
datNest
datNest = dat %>% group_nest(cluster.name)
datNest = dat %>% group_nest(d$cluster)
datNest = dat %>% group_nest(.data$cluster)
datNest
source('~/Dropbox/Personal computer/Independent studies/MetaUtility R package/MetaUtility/R/functions.R', echo=TRUE)
# accounting for clusters: CI = [0%, 61%]
Phat = prop_stronger( q = -0.1,
ci.level = 0.95,
estimate.method = "calibrated",
ci.method = "calibrated",
dat = d,
yi.name = "yi",
vi.name = "vyi",
tail = "below",
cluster.name = "paper")
Phat
check()
check()
d_to_logRR = Vectorize( function( d, se ) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
return( list(logRR = logRR, varlogRR = varlogRR) )
}, vectorize.args = c("d", "se") }
d_to_logRR = Vectorize( function( d, se ) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
return( list(logRR = logRR, varlogRR = varlogRR) )
}, vectorize.args = c("d", "se") )
d_to_logRR(.5)
d_to_logRR = Vectorize( function( d, se = NAs) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
return( list(logRR = logRR, varlogRR = varlogRR) )
}, vectorize.args = c("d", "se") )
d_to_logRR = Vectorize( function( d, se = NA) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
return( list(logRR = logRR, varlogRR = varlogRR) )
}, vectorize.args = c("d", "se") )
d_to_logRR(.5)
d_to_logRR(.5, .3)
d_to_logRR = Vectorize( function( d, se = NA) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
return( data.frame(logRR = logRR, varlogRR = varlogRR) )
}, vectorize.args = c("d", "se") )
d_to_logRR(.5, .3)
d_to_logRR = Vectorize( function( d, se = NA) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
return( data.frame(logRR, varlogRR) )
}, vectorize.args = c("d", "se") )
d_to_logRR(.5, .3)
#'         N = 100 )
d_to_logRR = Vectorize( function( d, se = NA) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
browser()
return( data.frame(logRR, varlogRR) )
}, vectorize.args = c("d", "se") )
d_to_logRR(.5, .3)
varlogRR
data.frame(logRR, varlogRR)
#'         N = 100 )
d_to_logRR = function( d, se = NA) {
# simplified the math
# Hasselblad conversion to log-OR followed by TVW's square-root transformation
#  to RR so that we can imagine dichotomizing near median
logRR = log( sqrt( exp( d * pi / sqrt(3) ) ) )
# delta method:
# derivative of log( sqrt( exp(c*d) ) ) wrt d = pi/(2*sqrt(3))
# so squared derivative is pi^2 / 12
varlogRR = ( pi^2 / 12 ) * se^2
browser()
return( data.frame(logRR, varlogRR) )
}
d_to_logRR(.5, .3)
source('~/Dropbox/Personal computer/Independent studies/MetaUtility R package/MetaUtility/R/functions.R', echo=TRUE)
d_to_logRR(.5, .3)
d_to_logRR(.5)
d_to_logRR(c(.5,.4))
d_to_logRR(c(.5,.4), .3)
d_to_logRR(c(.5,.4), c(.3, 2)
)
d_to_logRR(.5, .21)
document()
?d_to_logRR
document()
chekc()
check()
check()
check()
dplyr::unnest
unnest
?unnest
?group_nest
check()
check()
check()
check()
?d_to_logRR
?prop_stronger
document()
?prop_stronger
build()
check()
devtools::check_win_devel()
build()
library(devtools)
check()
document()
build()
check()
check()
13
13/8
5.69/2
library(dplyr)
# turns a vector of Qualtrics completion times (e.g., "8/26/20 7:38") into date objects
dateify = function(x) {
# need to split on space because there are also times in the strings
x2 = strsplit(x, " ")
# keep only the first part (date)
x3 = unlist( lapply(x2, function(.x) .x[[1]]) )
as.Date(x3, "%Y-%m-%d")
}
# dateify("8/26/20 7:38")
setwd("~/Dropbox/Personal computer/Workouts/Workout log (new)/2019")
d = read.csv("data.csv")
d$date = dateify(d$StartDate)
# keep only this week
endDate = as.Date("2021-03-07", "%Y-%m-%d")
d = d[ !is.na(d$date) & d$date <= endDate & d$date >= endDate - 6, ]
# sanity check
d$date
length(d$date)
# means
d %>% select( workout, sleep, energy, mood ) %>%
summarise_all( function(x) mean( as.numeric(x) ) )
d$week
d$mood
# workout hrs
d %>% select( cardio, lift ) %>%
summarise_all( function(x) sum( as.numeric(x) ) / 60 )
endDate = as.Date("2021-03-14", "%Y-%m-%d")
d = d[ !is.na(d$date) & d$date <= endDate & d$date >= endDate - 6, ]
# sanity check
d$date
length(d$date)
# means
d %>% select( workout, sleep, energy, mood ) %>%
summarise_all( function(x) mean( as.numeric(x) ) )
d
d$EndDate
setwd("~/Dropbox/Personal computer/Workouts/Workout log (new)/2019")
d = read.csv("data.csv")
d$date = dateify(d$StartDate)
d$date
# keep only this week
endDate = as.Date("2021-03-14", "%Y-%m-%d")
d[ !is.na(d$date) & d$date <= endDate & d$date >= endDate - 6, ]
d = d[ !is.na(d$date) & d$date <= endDate & d$date >= endDate - 6, ]
# sanity check
d$date
length(d$date)
# means
d %>% select( workout, sleep, energy, mood ) %>%
summarise_all( function(x) mean( as.numeric(x) ) )
# workout hrs
d %>% select( cardio, lift ) %>%
summarise_all( function(x) sum( as.numeric(x) ) / 60 )
# mile equivalent
pace = 8.5
sum( as.numeric(d$cardio) ) / pace
check()
library(devtools)
check()
setwd("~/Dropbox/Personal computer/Independent studies/MetaUtility R package/MetaUtility")
check()
build()
check()
document()
build()
check()
build()
check()
check_win()
library(devtools)
check_win()
check_win_devel()
5*9
5*7 + 2*4
.5^3
library(dplyr)
# turns a vector of Qualtrics completion times (e.g., "8/26/20 7:38") into date objects
dateify = function(x) {
# need to split on space because there are also times in the strings
x2 = strsplit(x, " ")
# keep only the first part (date)
x3 = unlist( lapply(x2, function(.x) .x[[1]]) )
as.Date(x3, "%Y-%m-%d")
}
# dateify("8/26/20 7:38")
setwd("~/Dropbox/Personal computer/Workouts/Workout log (new)/2019")
d = read.csv("data.csv")
d$date = dateify(d$StartDate)
# keep only this week
endDate = as.Date("2021-03-14", "%Y-%m-%d")
d = d[ !is.na(d$date) & d$date <= endDate & d$date >= endDate - 6, ]
# sanity check
d$date
expect_equal( length(d$date), 7 )
library(testthat)
# turns a vector of Qualtrics completion times (e.g., "8/26/20 7:38") into date objects
dateify = function(x) {
# need to split on space because there are also times in the strings
x2 = strsplit(x, " ")
# keep only the first part (date)
x3 = unlist( lapply(x2, function(.x) .x[[1]]) )
as.Date(x3, "%Y-%m-%d")
}
# dateify("8/26/20 7:38")
setwd("~/Dropbox/Personal computer/Workouts/Workout log (new)/2019")
d = read.csv("data.csv")
d$date = dateify(d$StartDate)
# keep only this week
endDate = as.Date("2021-03-21
", "%Y-%m-%d")
# for taking means of this week
library(dplyr)
library(testthat)
# turns a vector of Qualtrics completion times (e.g., "8/26/20 7:38") into date objects
dateify = function(x) {
# need to split on space because there are also times in the strings
x2 = strsplit(x, " ")
# keep only the first part (date)
x3 = unlist( lapply(x2, function(.x) .x[[1]]) )
as.Date(x3, "%Y-%m-%d")
}
# dateify("8/26/20 7:38")
setwd("~/Dropbox/Personal computer/Workouts/Workout log (new)/2019")
d = read.csv("data.csv")
d$date = dateify(d$StartDate)
# keep only this week
endDate = as.Date("2021-03-21", "%Y-%m-%d")
d = d[ !is.na(d$date) & d$date <= endDate & d$date >= endDate - 6, ]
# sanity check
d$date
expect_equal( length(d$date), 7 )
# means
d %>% select( workout, sleep, energy, mood ) %>%
summarise_all( function(x) mean( as.numeric(x) ) )
# workout hrs
d %>% select( cardio, lift ) %>%
summarise_all( function(x) sum( as.numeric(x) ) / 60 )
# mile equivalent
pace = 8.5
sum( as.numeric(d$cardio) ) / pace
# means
d %>% select( workout, sleep, energy, mood ) %>%
summarise_all( function(x) mean( as.numeric(x) ) )
